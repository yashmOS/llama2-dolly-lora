# Base & IO
model_name: meta-llama/Llama-2-7b-hf
output_dir: outputs/checkpoints
cache_dir: ./hf_cache

# Data
processed_dir: data/processed
train_file: data/processed/train.jsonl
eval_file: data/processed/val.jsonl
test_file: data/processed/test.jsonl
max_seq_length: 1024

# Training
learning_rate: 2.0e-4
weight_decay: 0.0
num_train_epochs: 3
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 16
lr_scheduler_type: cosine
warmup_ratio: 0.03
logging_steps: 10
save_strategy: epoch
evaluation_strategy: epoch
seed: 42
bf16: true   # use bf16 when available; falls back to fp16 automatically

# LoRA (QLoRA style: 4-bit base + LoRA adapters)
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
target_modules:
  - q_proj
  - v_proj
  - k_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

# 4-bit quantization (bitsandbytes)
load_in_4bit: true
bnb_4bit_use_double_quant: true
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: bfloat16
